# -*- coding: utf-8 -*-
"""Houston_Crime_Data_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I-3Ge8zqrUpybQgy7wigeEn6DWXn9XXx

### Read the 6 Datasets from 2019-2024
"""

import pandas as pd
import numpy as np

df_2024= pd.read_excel('/content/sample_data/NIBRSPublicView2024.xlsx')

df_2024.head()

df_2023= pd.read_excel('/content/sample_data/NIBRSPublicView2023.xlsx')

df_2023.head()

pip install pyxlsb

df_2022 = pd.read_excel('/content/sample_data/NIBRSPublicView2022.xlsb')

df_2022.head()

df_2021 = pd.read_excel('/content/sample_data/NIBRSPublicView2021.xlsb')

df_2021.head()

df_2020 = pd.read_excel('/content/sample_data/NIBRSPublicView2020.xlsb')

df_2020.head()

df_2019 = pd.read_excel('/content/sample_data/NIBRSPublicView2019.xlsb')

df_2019.head()

"""### Converting numerical data representation to datetime for dataset from 2022-2019"""

# Convert numerical date representation to datetime
df_2022['RMSOccurrenceDate'] = pd.to_datetime(df_2022['RMSOccurrenceDate'], unit='D', origin='1899-12-30')
df_2022.head()

# Convert numerical date representation to datetime
df_2021['RMSOccurrenceDate'] = pd.to_datetime(df_2021['RMSOccurrenceDate'], unit='D', origin='1899-12-30')
df_2021.head()

# Convert numerical date representation to datetime
df_2020['RMSOccurrenceDate'] = pd.to_datetime(df_2020['RMSOccurrenceDate'], unit='D', origin='1899-12-30')
df_2020.head()

# Convert numerical date representation to datetime
df_2019['RMSOccurrenceDate'] = pd.to_datetime(df_2019['RMSOccurrenceDate'], unit='D', origin='1899-12-30')
df_2019.head()

"""### Concatenate all the dataframes to single dataset."""

# Concatenate multiple DataFrames
df1 = pd.concat([df_2019, df_2020, df_2021, df_2022, df_2023, df_2024])

"""### Taking only rows with city a Houston"""

df = df1[df1['City']=='HOUSTON']

df1.info()

df1.shape

df.info()

df.shape

"""### Checking the info for all datasets and final dataset"""

df.info()

df_2024.info()

df_2023.info()

df_2022.info()

df_2021.info()

df_2020.info()

df_2019.info()

"""### Feature Engineering"""

# Extract month and year
df['month'] = df['RMSOccurrenceDate'].dt.month
df['year'] = df['RMSOccurrenceDate'].dt.year

# Drop specified columns
columns_to_drop = ['StreetNo', 'StreetType', 'Suffix']
df.drop(columns=columns_to_drop, inplace=True)

# Convert non-NaN values to integers after extracting first 5 characters
df['ZIPCode'] = df['ZIPCode'].apply(lambda x: int(str(x)[:5]) if pd.notna(x) else np.nan).astype('Int64')

# Check the data type after conversion
print(df['ZIPCode'].dtype)

"""### Checking info on final dataset after feature engineering"""

df.info()

df.head()

"""### Checking for missing values"""

# Check the number of missing values in each column
missing_values = df.isnull().sum()

# Print the results elegantly
print("Missing Values per Column:")
for column, count in missing_values.items():
    print(f"{column}: {count}")

"""### Adding Region column to DataFrame using the Beat column"""

region ={
    'Airport-Hoby Division - District 23':['23J40','23J50'],
    'Airport-IAH Division - District 21':['21I10','21I40','21I50','21I60','21I70'],
    'Central Division - District 1,2':['1A20','1A30','1A40','1A50','2A10','2A20','2A30','2A40','2A50','2A60'],
    'Clear Lake Division - District 12':['12D10','12D20','12D30','12D40','12D50','12D60','12D70'],
    'Downtown Division - District 1':['1A10'],
    'Eastside Division - District 1':['11H10','11H20','11H30','11H40','11H50'],
    'Kingwod Division - District 24':['24C10','24C20','24C30','24C40','24C50','24C60'],
    'Midwest Division - District 18':['18F10','18F20','18F30','18F40','18F50','18F60'],
    'NorthBelt Division - District 22':['22B10','22B20','22B30','22B40'],
    'North Division - District 3,6':['3B10','3B30','3B40','3B50','6B10','6B20','6B30','6B40','6B60'],
    'Northeast Division - District 7,8,9':['7C10','7C20','7C30','7C40','8C10','8C20','8C30','8C40','8C50','8C60','9C10','9C20','9C30','9C40'],
    'Northwest Division - District 4,5':['4F10','4F20','4F30','5F10','5F20','5F30','5F40'],
    'South Central Division - District 10':['10H10','10H20','10H30','10H40','10H50','10H60','10H70','10H80'],
    'South Gesner Division - District 17':['17E10','17E20','17E30','17E40'],
    'Southeast Division - District 13,14':['13D10','13D20','13D30','13D40','14D10','14D20','14D30','14D40','14D50'],
    'Southwest Division - District 15,16':['15E10','15E20','15E30','15E40','16E10','16E20','16E30','16E40'],
    'Westside Division - District 19,20':['19G10','19G20','19G30','19G40','19G50','20G10','20G20','20G30','20G40','20G50','20G60','20G70','20G80'],
    'Harris County Sheriffs Office':['HCSO'],
    'Harris County Constableâ€™s Office Precinct 5':['HCC5'],
    'OOJ':['OOJ']
}

def get_region(beat):
    for key, value in region.items():
        if beat in value:
            return key
    return None

# Apply the function to create the 'region' column
df['Region'] = df['Beat'].apply(get_region)

"""### Checking for missing values"""

# Check the number of missing values in each column
missing_values = df.isnull().sum()

# Print the results elegantly
print("Missing Values per Column:")
for column, count in missing_values.items():
    print(f"{column}: {count}")

"""### Checking for duplicate rows in final dataframe."""

# Check for duplicate rows based on all columns
duplicate_rows = df[df.duplicated()]

if duplicate_rows.empty:
    print("No duplicate rows found.")
else:
    print("Duplicate rows found:\n", duplicate_rows)

"""### Checking for outliers"""

# Define a function to detect outliers based on IQR
def detect_outliers(column):
    Q1 = column.quantile(0.25)
    Q3 = column.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return (column < lower_bound) | (column > upper_bound)

# Apply the function to each numerical column
numerical_columns = df.select_dtypes(include=['int64', 'float64'])
outliers = numerical_columns.apply(detect_outliers)

# Count the number of outliers in each column
outlier_counts = outliers.sum()

# Print columns with outlier counts
print("Outlier counts:")
print(outlier_counts)

df.head()

df.info()

"""###Web scrape Houston Demographic data by Zip code"""

import pandas as pd
import numpy as np
from bs4 import BeautifulSoup
import requests

dataset = pd.read_excel('./zip_codes.xlsx')
dataset.head()

#Turn Zipcodes into ints
zipList =  dataset['ZipCode'].values.tolist()
zipList = [int(x) for x in zipList]

dataset['ZipCode'] = zipList
uniqueZips = zipList

url = 'https://worldpopulationreview.com/zip/77072'
page = requests.get(url)
soup = BeautifulSoup(page.content, "html.parser")

data = []

for zipCode in uniqueZips:
  dataList = []
  url = f'https://worldpopulationreview.com/zip/{zipCode}'
  page = requests.get(url)
  soup = BeautifulSoup(page.content, "html.parser")

  ZipCode = zipCode
  try:
    population = soup.div.text.split('Population by Race')[0].split('Zip Code ')[1][5:]
  except:
    population = np.nan
  try:
    blackPercent = soup.div.text.split('Black or African American')[1].split(',')[1].split('%')[0][3:]
  except:
    blackPercent = np.nan
  try:
    asianPercent = soup.div.text.split('Asian')[1].split(',')[1].split('%')[0][3:]
  except:
    asianPercent = np.nan

  try:
    whitePercent = soup.div.text.split('White')[1].split(',')[1].split('%')[0][3:]
  except:
    whitePercent = np.nan
  try:
    medianAge = soup.div.text.split('Age')[1].split('Total')[0]
  except:
    medianAge = np.nan
  try:
    ageDepRatio = soup.div.text.split('Age')[2].split('Dependency')[1]
  except:
    ageDepRatio = np.nan
  try:
    oldAgeDepRatio = soup.div.text.split('Age')[3].split('Old')[0].split('Ratio')[1]
  except:
    oldAgeDepRatio = np.nan
  try:
    childDepRatio = soup.div.text.split('Age')[4].split('Child')[0].split('Ratio')[1]
  except:
    childDepRatio = np.nan
  try:
    homeOwnershipPercent = soup.div.text.split('Rate of Home Ownership')[0].split('Average Household Size')[1].split('%')[0]
  except:
    homeOwnershipPercent = np.nan
  try:
    averageEarnings = soup.div.text.split('Average Earnings')[0].split('$')[1]
  except:
    averageEarnings = np.nan
  try:
    overallPovertyPercent = soup.div.text.split('Overall Poverty')[0].split('PovertyPoverty RateHispanic')[1].split('Native')[1].split('%')[1]
  except:
    overallPovertyPercent = np.nan
  try:
    unemploymentRate = soup.div.text.split('Unemployment Rate')[0].split('Employment Rate')[1]
  except:
    unemploymentRate = np.nan

  dataList = [ZipCode, population, blackPercent, asianPercent, whitePercent, medianAge, ageDepRatio, oldAgeDepRatio,
              childDepRatio, homeOwnershipPercent, averageEarnings, overallPovertyPercent, unemploymentRate]
  data.append(dataList)
  print(zipCode)

df = pd.DataFrame(data, columns = ['ZipCode', 'Population', 'BlackPercent', 'AsianPercent',
                                   'WhitePercent', 'MedianAge', 'AgeDepRatio', 'OldAgeDepRatio', 'ChildDepRatio', 'HomeOwnershipPercent',
                                   'AverageEarnings', 'PovertyPercent', 'UnemploymentRate'])

#str to ints
df['BlackPercent'] = [float(x) for x in df['BlackPercent']]
df['AsianPercent'] = [float(x) for x in df['AsianPercent']]
df['WhitePercent'] = [float(x) for x in df['WhitePercent']]
df['MedianAge'] = [float(x) for x in df['MedianAge']]
df['AgeDepRatio'] = [float(x) for x in df['AgeDepRatio']]
df['OldAgeDepRatio'] = [float(x) for x in df['OldAgeDepRatio']]
df['ChildDepRatio'] = [float(x) for x in df['ChildDepRatio']]
df['HomeOwnershipPercent'] = [float(x) for x in df['HomeOwnershipPercent']]

df

"""### Feature Engineering based on web scrapped data"""

df_demo1 = pd.read_excel('/content/sample_data/ZipCodeDemographics2024_Part1.xlsx')
df_demo1.drop(columns=['Unnamed: 0'], inplace=True)
df_demo1.head()

df_demo1.shape

df_demo2 = pd.read_excel('/content/sample_data/ZipCodeDemographics2024_Part2.xlsx')
df_demo2.drop(columns=['Unnamed: 0'], inplace=True)
df_demo2.head()

df_demo2.shape

df_demo = pd.concat([df_demo1, df_demo2])

df_demo.head()

df_demo.shape

# Get unique values of 'ZipCode' column in each DataFrame
demo_zip_codes = set(df_demo['ZipCode'].unique())
df_zip_codes = set(df['ZIPCode'].unique())

# Find common and different unique values
common_zip_codes = demo_zip_codes.intersection(df_zip_codes)
different_zip_codes = demo_zip_codes.symmetric_difference(df_zip_codes)

# Print the counts
print("Number of common unique zip codes:", len(common_zip_codes))
print("Number of different unique zip codes:", len(different_zip_codes))

len(demo_zip_codes)

len(df_zip_codes)

# Rename a single column
df_demo.rename(columns={'ZipCode': 'ZIPCode'}, inplace=True)

df_demo.head()

df.head()

# Check the number of missing values in each column
missing_values = df.isnull().sum()

# Print the results elegantly
print("Missing Values per Column:")
for column, count in missing_values.items():
    print(f"{column}: {count}")

df.info()

# Convert NaN values in the ZIPCode column of df to 00000
df['ZIPCode'].fillna(0, inplace=True)

# Create a DataFrame with all NaN values and 00000 as the ZIPCode
nan_row = pd.DataFrame([[0] + [np.nan] * (len(df_demo.columns) - 1)], columns=['ZIPCode'] + list(df_demo.columns[1:]))

df_demo_with_nan = pd.concat([df_demo, nan_row], ignore_index=True)

# Append the NaN row to df_demo
#df_demo_with_nan = df_demo.append(nan_row, ignore_index=True)

# Merge the dataframes on the 'ZIPCode' column
df_merged = df.merge(df_demo_with_nan, on='ZIPCode', how='left')

# Display the merged dataframe
df_merged

df_merged.info()

# Convert 0 values in ZIPCode column back to NaN
df_merged['ZIPCode'].replace(0, np.nan, inplace=True)

# Check the number of missing values in each column
missing_values = df_merged.isnull().sum()

# Print the results elegantly
print("Missing Values per Column:")
for column, count in missing_values.items():
    print(f"{column}: {count}")

# Drop all rows where 'Beat' column is NaN
df_merged.dropna(subset=['Beat'], inplace=True)

# Check the number of missing values in each column
missing_values = df_merged.isnull().sum()

# Print the results elegantly
print("Missing Values per Column:")
for column, count in missing_values.items():
    print(f"{column}: {count}")

# Save the cleaned DataFrame to an Excel file
df_merged.to_csv('final_dataset_1.csv', index=False)

data = df_merged

# Example data preparation steps (adjust as necessary)
# Ensure RMSOccurrenceDate is a datetime type for any time series analysis
data['RMSOccurrenceDate'] = pd.to_datetime(data['RMSOccurrenceDate'])
data['DayOfWeek'] = data['RMSOccurrenceDate'].dt.day_name()

import plotly.express as px

# Plot 1: Daily Incident Count Over Time
fig_ts = px.line(data.groupby('RMSOccurrenceDate').size().reset_index(name='Incident Count'),
                 x='RMSOccurrenceDate', y='Incident Count',
                 title='Daily Incident Count Over Time')
fig_ts.show()

# Ensure data for geospatial plots is clean and available
# data_clean = data.dropna(subset=['MapLatitude', 'MapLongitude'])

# Plot 2: Map of Incidents
fig_map = px.scatter_mapbox(data.dropna(subset=['MapLatitude', 'MapLongitude']),
                            lat='MapLatitude', lon='MapLongitude',
                            color='NIBRSDescription',
                            title='Map of Incidents',
                            mapbox_style='open-street-map',
                            zoom=10)
fig_map.show()

# Plot 3: Incidents by Hour of the Day
fig_hour = px.histogram(data, x='RMSOccurrenceHour',
                        title='Incidents by Hour of the Day',
                        nbins=24)
fig_hour.update_xaxes(type='category')
fig_hour.show()

# Plot 4: Incident Types Distribution
fig_types = px.pie(data, names='NIBRSDescription', title='Incident Types Distribution')
fig_types.show()

# Plot 5: Number of Incidents by ZIP Code
fig_zip = px.histogram(data, x='ZIPCode',
                       title='Number of Incidents by ZIP Code')
fig_zip.show()

# Plot 6: Incidents by Day of the Week
fig_weekday = px.bar(data['DayOfWeek'].value_counts().reset_index(),
                     x='index', y='DayOfWeek',
                     labels={'index': 'Day of the Week', 'DayOfWeek': 'Number of Incidents'},
                     title='Incidents by Day of the Week')
fig_weekday.show()

# Plot 7: Heatmap of Incidents by Hour and Day of the Week
heatmap_data = data.pivot_table(index='DayOfWeek', columns='RMSOccurrenceHour', values='Incident', aggfunc='count').fillna(0)
fig_heatmap = px.imshow(heatmap_data,
                        labels=dict(x="Hour of Day", y="Day of Week", color="Incident Count"),
                        title='Heatmap of Incidents by Hour and Day of the Week')
fig_heatmap.show()

# Plot 8: Trend of Specific Incident Type Over Time
# Example for "Aggravated Assault" (adjust 'NIBRSDescription' as needed)
assault_data = data[data['NIBRSDescription'] == 'Aggravated Assault']
fig_assault_trend = px.line(assault_data.groupby('RMSOccurrenceDate').size().reset_index(name='Count'),
                            x='RMSOccurrenceDate', y='Count',
                            title='Trend of Aggravated Assault Incidents Over Time')
fig_assault_trend.show()

# Plot 9: Offense Count by Premise Type
fig_premise = px.histogram(data, x='Premise', title='Offense Count by Premise Type')
fig_premise.show()

# Plot 10: Geospatial Scatter Plot of Incidents
fig_geo_scatter = px.scatter(data, x='MapLongitude', y='MapLatitude', color='NIBRSDescription',
                             title='Geospatial Scatter Plot of Incidents',
                             labels={'MapLongitude': 'Longitude', 'MapLatitude': 'Latitude'})
fig_geo_scatter.show()